{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d37dba",
   "metadata": {
    "papermill": {
     "duration": 0.005767,
     "end_time": "2022-08-26T20:41:03.306513",
     "exception": false,
     "start_time": "2022-08-26T20:41:03.300746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Converting PyTorch CLIP model to ONNX\n",
    "\n",
    "In this notebook I'd like to show how to successfully convert PyTorch CLIP model to ONNX, simplify it, load into onnxruntime and get a 25% speed boost on CPU. \n",
    "\n",
    "Why? In the context of this particular competition - no reason, I don't think it's profitable to inference models on CPU rather than GPU. However, outside of Kaggle it might become a necessity to inference models on CPU. \n",
    "\n",
    "Another reason:\n",
    "\n",
    "<center><img src=\"https://i.imgflip.com/1ch27o.jpg\" alt=\"drawing\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3838e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:03.316976Z",
     "iopub.status.busy": "2022-08-26T20:41:03.316363Z",
     "iopub.status.idle": "2022-08-26T20:41:23.318954Z",
     "shell.execute_reply": "2022-08-26T20:41:23.317508Z"
    },
    "papermill": {
     "duration": 20.011374,
     "end_time": "2022-08-26T20:41:23.322026",
     "exception": false,
     "start_time": "2022-08-26T20:41:03.310652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-lcsch6nq\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-lcsch6nq\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting onnxruntime\r\n",
      "  Downloading onnxruntime-1.12.1-cp37-cp37m-manylinux_2_27_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting onnx-simplifier\r\n",
      "  Downloading onnx_simplifier-0.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting ftfy\r\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.11.0+cpu)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.12.0+cpu)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from onnxruntime) (1.21.6)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from onnxruntime) (21.3)\r\n",
      "Collecting coloredlogs\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.7/site-packages (from onnxruntime) (1.10.1)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from onnxruntime) (3.19.4)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.7/site-packages (from onnxruntime) (1.12)\r\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.7/site-packages (from onnx-simplifier) (1.12.0)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.7/site-packages (from onnx-simplifier) (12.1.0)\r\n",
      "Collecting humanfriendly>=9.1\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.7/site-packages (from onnx->onnx-simplifier) (4.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->onnxruntime) (3.0.9)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich->onnx-simplifier) (2.12.0)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich->onnx-simplifier) (0.9.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.7/site-packages (from sympy->onnxruntime) (1.2.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.28.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.26.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2022.6.15)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.1.0)\r\n",
      "Building wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=a0d25928a1caea3b247438c72ef8458ed4d80afc7c25cf428d3385d245622cb1\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-msmyd_ya/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: humanfriendly, ftfy, onnx-simplifier, coloredlogs, onnxruntime, clip\r\n",
      "Successfully installed clip-1.0 coloredlogs-15.0.1 ftfy-6.1.1 humanfriendly-10.0 onnx-simplifier-0.4.7 onnxruntime-1.12.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git onnxruntime onnx-simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ea92cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:23.338532Z",
     "iopub.status.busy": "2022-08-26T20:41:23.338132Z",
     "iopub.status.idle": "2022-08-26T20:41:25.801398Z",
     "shell.execute_reply": "2022-08-26T20:41:25.800342Z"
    },
    "papermill": {
     "duration": 2.474648,
     "end_time": "2022-08-26T20:41:25.804006",
     "exception": false,
     "start_time": "2022-08-26T20:41:23.329358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbkaishui/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import time\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxsim import simplify\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a0fdb2",
   "metadata": {
    "papermill": {
     "duration": 0.006381,
     "end_time": "2022-08-26T20:41:25.817076",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.810695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399b00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:25.831971Z",
     "iopub.status.busy": "2022-08-26T20:41:25.831286Z",
     "iopub.status.idle": "2022-08-26T20:41:25.838191Z",
     "shell.execute_reply": "2022-08-26T20:41:25.837170Z"
    },
    "papermill": {
     "duration": 0.017081,
     "end_time": "2022-08-26T20:41:25.840693",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.823612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLIP_BACKBONE = 'RN50'\n",
    "CLIP_BACKBONE = 'ViT-B/32'\n",
    "CLIP_ONNX_EXPORT_PATH = 'clip_resnet.onnx'\n",
    "CLIP_ONNX_EXPORT_PATH_SIMP = 'clip_resnet_simplified.onnx'\n",
    "\n",
    "ONNX_INPUT_NAMES = [\"IMAGE\", \"TEXT\"]\n",
    "ONNX_OUTPUT_NAMES = [\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"]\n",
    "ONNX_DYNAMIC_AXES = {\n",
    "    \"IMAGE\": {\n",
    "        0: \"image_batch_size\",\n",
    "    },\n",
    "    \"TEXT\": {\n",
    "        0: \"text_batch_size\",\n",
    "    },\n",
    "    \"LOGITS_PER_IMAGE\": {\n",
    "        0: \"image_batch_size\",\n",
    "        1: \"text_batch_size\",\n",
    "    },\n",
    "    \"LOGITS_PER_TEXT\": {\n",
    "        0: \"text_batch_size\",\n",
    "        1: \"image_batch_size\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b08b40ec",
   "metadata": {
    "papermill": {
     "duration": 0.006627,
     "end_time": "2022-08-26T20:41:25.854116",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.847489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helpers \n",
    "\n",
    "Define some basic helper functions to easily load, export, and benchmark models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ea69d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:25.869325Z",
     "iopub.status.busy": "2022-08-26T20:41:25.868937Z",
     "iopub.status.idle": "2022-08-26T20:41:25.878213Z",
     "shell.execute_reply": "2022-08-26T20:41:25.877003Z"
    },
    "papermill": {
     "duration": 0.019531,
     "end_time": "2022-08-26T20:41:25.880387",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.860856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_mean_time_no_warmup(\n",
    "    func, \n",
    "    func_inputs, \n",
    "    num_iters=250\n",
    ") -> float:\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_iters):\n",
    "        func(*func_inputs)\n",
    "    return (time.perf_counter() - start_time) / num_iters\n",
    "\n",
    "\n",
    "def load_clip(backbone='RN50') -> Tuple[clip.model.CLIP, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    pytorch_model, pre = clip.load(backbone)\n",
    "    npx = pytorch_model.visual.input_resolution\n",
    "    print(f\"npx is {npx}\")\n",
    "    dummy_image = torch.randn(10, 3, npx, npx)\n",
    "    dummy_texts = clip.tokenize([\"quick brown fox\", \"lorem ipsum\"])\n",
    "    \n",
    "    return pytorch_model, (dummy_image, dummy_texts)\n",
    "\n",
    "\n",
    "def export_onnx(\n",
    "    model, \n",
    "    inputs, \n",
    "    input_names,\n",
    "    output_names,\n",
    "    dynamic_axes,\n",
    "    export_path\n",
    ") -> None:\n",
    "    torch.onnx.export(\n",
    "        model=model, \n",
    "        args=inputs, \n",
    "        f=export_path, \n",
    "        export_params=True,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        opset_version=14,\n",
    "        dynamic_axes=dynamic_axes\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d847e128",
   "metadata": {
    "papermill": {
     "duration": 0.006294,
     "end_time": "2022-08-26T20:41:25.893477",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.887183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load and export\n",
    "Load PyTorch version of CLIP and export it to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80eb044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:25.908909Z",
     "iopub.status.busy": "2022-08-26T20:41:25.907980Z",
     "iopub.status.idle": "2022-08-26T20:41:58.744743Z",
     "shell.execute_reply": "2022-08-26T20:41:58.743491Z"
    },
    "papermill": {
     "duration": 32.847844,
     "end_time": "2022-08-26T20:41:58.747966",
     "exception": false,
     "start_time": "2022-08-26T20:41:25.900122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npx is 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbkaishui/anaconda3/envs/py39/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:4189: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pytorch_model, dummy_input = load_clip(backbone=CLIP_BACKBONE)\n",
    "pytorch_model.eval()\n",
    "\n",
    "export_onnx(\n",
    "    model=pytorch_model,\n",
    "    inputs=dummy_input,\n",
    "    input_names=ONNX_INPUT_NAMES,\n",
    "    output_names=ONNX_OUTPUT_NAMES,\n",
    "    dynamic_axes=ONNX_DYNAMIC_AXES,\n",
    "    export_path=CLIP_ONNX_EXPORT_PATH,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4cc60c5",
   "metadata": {
    "papermill": {
     "duration": 0.010088,
     "end_time": "2022-08-26T20:41:58.769145",
     "exception": false,
     "start_time": "2022-08-26T20:41:58.759057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check + simplify\n",
    "Make sure ONNX model exported successfully and run onnx-simplifier on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6584734b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:41:58.792174Z",
     "iopub.status.busy": "2022-08-26T20:41:58.791356Z",
     "iopub.status.idle": "2022-08-26T20:43:05.148411Z",
     "shell.execute_reply": "2022-08-26T20:43:05.146346Z"
    },
    "papermill": {
     "duration": 66.374603,
     "end_time": "2022-08-26T20:43:05.153757",
     "exception": false,
     "start_time": "2022-08-26T20:41:58.779154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run checks\n",
    "onnx_model = onnx.load(CLIP_ONNX_EXPORT_PATH)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# run additional checks and simplify\n",
    "model_simp, check = simplify(onnx_model, skip_fuse_bn=True)\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, CLIP_ONNX_EXPORT_PATH_SIMP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99cf504a",
   "metadata": {
    "papermill": {
     "duration": 0.009902,
     "end_time": "2022-08-26T20:43:05.179778",
     "exception": false,
     "start_time": "2022-08-26T20:43:05.169876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## onnxruntime\n",
    "\n",
    "Load ONNX model into onnxruntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09476fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:43:05.202464Z",
     "iopub.status.busy": "2022-08-26T20:43:05.202010Z",
     "iopub.status.idle": "2022-08-26T20:43:06.090457Z",
     "shell.execute_reply": "2022-08-26T20:43:06.089208Z"
    },
    "papermill": {
     "duration": 0.903853,
     "end_time": "2022-08-26T20:43:06.093788",
     "exception": false,
     "start_time": "2022-08-26T20:43:05.189935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(CLIP_ONNX_EXPORT_PATH_SIMP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d3e842",
   "metadata": {
    "papermill": {
     "duration": 0.009994,
     "end_time": "2022-08-26T20:43:06.114186",
     "exception": false,
     "start_time": "2022-08-26T20:43:06.104192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference\n",
    "\n",
    "Run inference for both PyTorch and ONNX version to verife that results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f1d647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:43:06.136306Z",
     "iopub.status.busy": "2022-08-26T20:43:06.135903Z",
     "iopub.status.idle": "2022-08-26T20:43:08.315412Z",
     "shell.execute_reply": "2022-08-26T20:43:08.314455Z"
    },
    "papermill": {
     "duration": 2.193581,
     "end_time": "2022-08-26T20:43:08.318055",
     "exception": false,
     "start_time": "2022-08-26T20:43:06.124474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch output: (tensor([[20.0043, 24.3510],\n",
      "        [19.5753, 24.7952],\n",
      "        [19.9016, 25.3230],\n",
      "        [20.2808, 24.6213],\n",
      "        [19.5199, 24.3743],\n",
      "        [19.7097, 24.8914],\n",
      "        [20.0616, 24.7587],\n",
      "        [19.5600, 24.4032],\n",
      "        [20.2513, 24.6054],\n",
      "        [19.4823, 24.6365]]), tensor([[20.0043, 19.5753, 19.9016, 20.2808, 19.5199, 19.7097, 20.0616, 19.5600,\n",
      "         20.2513, 19.4823],\n",
      "        [24.3510, 24.7952, 25.3230, 24.6213, 24.3743, 24.8914, 24.7587, 24.4032,\n",
      "         24.6054, 24.6365]]))\n",
      "\n",
      "ONNX output: [array([[20.00431 , 24.351057],\n",
      "       [19.57537 , 24.795254],\n",
      "       [19.901592, 25.322998],\n",
      "       [20.280855, 24.621252],\n",
      "       [19.519897, 24.37428 ],\n",
      "       [19.709751, 24.891382],\n",
      "       [20.061575, 24.758688],\n",
      "       [19.560017, 24.403189],\n",
      "       [20.251362, 24.605425],\n",
      "       [19.482271, 24.636532]], dtype=float32), array([[20.00431 , 19.57537 , 19.901592, 20.280855, 19.519897, 19.709751,\n",
      "        20.061575, 19.560017, 20.251362, 19.482271],\n",
      "       [24.351057, 24.795254, 25.322998, 24.621252, 24.37428 , 24.891382,\n",
      "        24.758688, 24.403189, 24.605425, 24.636532]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_output = pytorch_model(*dummy_input)\n",
    "onnx_output = ort_sess.run(ONNX_OUTPUT_NAMES, {\"IMAGE\": dummy_input[0].numpy(), \"TEXT\": dummy_input[1].numpy()})\n",
    "\n",
    "assert all([torch.allclose(pt_pred, torch.tensor(onnx_pred)) for pt_pred, onnx_pred in zip(pytorch_output, onnx_output)])\n",
    "\n",
    "print(f'Pytorch output: {pytorch_output}\\n\\nONNX output: {onnx_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0704e8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:43:08.341680Z",
     "iopub.status.busy": "2022-08-26T20:43:08.340776Z",
     "iopub.status.idle": "2022-08-26T20:52:16.995641Z",
     "shell.execute_reply": "2022-08-26T20:52:16.993784Z"
    },
    "papermill": {
     "duration": 548.690201,
     "end_time": "2022-08-26T20:52:17.019052",
     "exception": false,
     "start_time": "2022-08-26T20:43:08.328851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch mean time: 1.401 sec\n",
      "ONNX Runtime mean time: 0.723 sec\n",
      "Boost from PT -> ONNX (%) 48.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_mean_time = measure_mean_time_no_warmup(func=pytorch_model, func_inputs=dummy_input)\n",
    "onnx_runtime_mean_time = measure_mean_time_no_warmup(func=ort_sess.run, func_inputs=([\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], {\"IMAGE\": dummy_input[0].numpy(), \"TEXT\": dummy_input[1].numpy()}))\n",
    "\n",
    "print(f'PyTorch mean time: {round(pytorch_mean_time, 3)} sec\\nONNX Runtime mean time: {round(onnx_runtime_mean_time, 3)} sec\\nBoost from PT -> ONNX (%) {100*round(1 - onnx_runtime_mean_time/pytorch_mean_time, 2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdbf3ae6",
   "metadata": {
    "papermill": {
     "duration": 0.01325,
     "end_time": "2022-08-26T20:52:17.047558",
     "exception": false,
     "start_time": "2022-08-26T20:52:17.034308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Where to go next?\n",
    "\n",
    "The best course of action would be to take the converted model and load it into OpenVino for additional speed-ups. However, I personally couldn't do it due to the fact that some layers of CLIP don't seem to be supported by OpenVino. In case you manage to make it work - please, let me know, I'm curious about future improvements. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e19d1726",
   "metadata": {
    "papermill": {
     "duration": 0.022206,
     "end_time": "2022-08-26T20:52:17.093441",
     "exception": false,
     "start_time": "2022-08-26T20:52:17.071235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://i.pinimg.com/originals/06/82/e2/0682e26f337825b366e8e3e3e0003ad1.jpg\" alt=\"drawing\" width=\"450\"/></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.375394,
   "end_time": "2022-08-26T20:52:19.556559",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-26T20:40:54.181165",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
